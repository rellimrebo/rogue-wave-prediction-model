!pip install netCDF4 pandas
!pip install pyarrow
!pip install fastparquet
import numpy as np
import pandas as pd
import netCDF4
from scipy.signal import argrelextrema
import os
import datetime
import calendar
import math

# Configuration parameters and helper functions (as previously defined)
stn = '067'
deploy = '17'
start_date = '05/01/2008 00:00'  # MM/DD/YYYY HH:MM
end_date = '05/31/2008 23:59'  # MM/DD/YYYY HH:MM
sample_rate = 1.28  # Example sample rate in Hz, adjust as needed based on dataset
qc_level = 2 # Filter data with qc flags above this number 

# Helper functions for timestamp conversion
def get_unix_timestamp(human_time, dateFormat):
    return int(calendar.timegm(datetime.datetime.strptime(human_time, dateFormat).timetuple()))

def get_human_timestamp(unix_timestamp, dateFormat):
    return datetime.datetime.utcfromtimestamp(int(unix_timestamp)).strftime(dateFormat)

# Load dataset
data_url = f'http://thredds.cdip.ucsd.edu/thredds/dodsC/cdip/archive/{stn}p1/{stn}p1_d{deploy}.nc'
nc = netCDF4.Dataset(data_url)
nc.set_auto_mask(False)

# Extracting variables
qc_flag = nc.variables['xyzFlagPrimary']
time_var = nc.variables['waveTime'][:]  # Assuming this is in UNIX timestamp format
waveHs = nc.variables['waveHs'][:]
start_time = nc.variables['xyzStartTime'][:].item()
WaveHeight = nc.variables['xyzZDisplacement'][:]
end_time = start_time + (len(WaveHeight) / sample_rate)

# Calculate UNIX timestamps for analysis period
unix_start = get_unix_timestamp(start_date, "%m/%d/%Y %H:%M")
unix_end = get_unix_timestamp(end_date, "%m/%d/%Y %H:%M")

# Calculate sub-second sample times and filter the data for the specified time range
sample_time = np.arange(start_time, end_time, 1 / sample_rate)
start_index = np.searchsorted(sample_time, unix_start)
end_index = np.searchsorted(sample_time, unix_end)
filtered_wave_heights = np.ma.masked_where(qc_flag[start_index:end_index] > qc_level, WaveHeight[start_index:end_index])
filtered_timestamps = sample_time[start_index:end_index]

# Mapping waveHs to the timestamps
waveHs_interval_index = np.searchsorted(time_var, filtered_timestamps, side='right') - 1
waveHs_mapped = waveHs[waveHs_interval_index]

# Data preparation for analysis
df = pd.DataFrame({
    'timestamp': filtered_timestamps,
    'xyzZDisplacement': filtered_wave_heights,
    'waveHs': waveHs_mapped
})

# Zero upcrossings, spike detection, and repetitive values check functions
def calculate_zero_upcrossings(xyzZDisplacement):
    upcrossings = np.where(np.diff(np.sign(xyzZDisplacement)) > 0)[0]
    if len(upcrossings) > 1:
        Tz = np.mean(np.diff(upcrossings)) / sample_rate
        Nz = len(upcrossings)
    else:
        Tz = 0
        Nz = 0
    return Nz, Tz

def sensor_range_exceeded(window, column_name='xyzZDisplacement', max_range=20.47):
    # Check if any absolute wave heights exceed the max_range
    exceeded = (window[column_name].abs() > max_range).any()
    
    return exceeded
  
def check_for_spikes(window, Sy):
    rate_of_change = np.abs(np.diff(window['xyzZDisplacement'].values)) / (1 / sample_rate)
    return np.any(rate_of_change > 2 * Sy)

def check_for_repetitive_values(window):
    return (window['WaveHeight'].rolling(window=10, min_periods=1).apply(lambda x: x.nunique() == 1)).any()

def prepare_and_classify_wave_data(df, sample_rate, waveHs_mapped, observation_window=20*60, advance_warning_time=5*60):
    minima_indices = argrelextrema(df['xyzZDisplacement'].values, np.less)[0]
    maxima_indices = argrelextrema(df['xyzZDisplacement'].values, np.greater)[0]
    
    df['class'] = 0  # Initialize all as non-rogue (class 0)

    rogue_wave_windows = []
    non_rogue_wave_windows = []

    # Classify rogue waves with 25 minutes warning time marked as class 1
    for min_idx in minima_indices:
        min_val = df.iloc[min_idx]['xyzZDisplacement']
        subsequent_max_idx = maxima_indices[maxima_indices > min_idx]
        if min_val < 0 and len(subsequent_max_idx) > 0:
            max_idx = subsequent_max_idx[0]
            wave_height = df.iloc[max_idx]['xyzZDisplacement'] - min_val
            if wave_height > 2 * waveHs_mapped[min_idx]:
                # Extend classification for 25 minutes before the rogue wave
                warning_start_idx = max(0, min_idx - int(advance_warning_time * sample_rate))
                warning_end_idx = min_idx + int((observation_window - advance_warning_time) * sample_rate)
                df.loc[warning_start_idx:warning_end_idx, 'class'] = 1
                rogue_wave_windows.append((warning_start_idx, warning_end_idx))

    # Apply quality checks uniformly to all windows
    for start_idx in range(len(df) - observation_window - advance_warning_time):
        window_df = df.iloc[start_idx:start_idx + observation_window + advance_warning_time]
        
        Nz, Tz = calculate_zero_upcrossings(window_df['xyzZDisplacement'].values)
        sigma = window_df['xyzZDisplacement'].std()
        Sy = 4 * np.pi * sigma * np.sqrt(2 * np.log(Nz)) / Tz

        if check_for_spikes(window_df, Sy, sample_rate) or sensor_range_exceeded(window_df):
            df.loc[start_idx:start_idx + observation_window + advance_warning_time, 'class'] = 9  # Mark as failed quality checks

    # Ensure the non-rogue wave dataset has the same size as the rogue wave dataset
    needed_non_rogue_windows = len(rogue_wave_windows)
    while needed_non_rogue_windows > len(non_rogue_wave_windows):
        potential_start = np.random.randint(0, len(df) - observation_window - advance_warning_time)
        potential_end = potential_start + observation_window + advance_warning_time
        
        # Check if window overlaps with rogue wave windows or failed quality checks
        if not any([start <= potential_start <= end or start <= potential_end <= end for start, end in rogue_wave_windows + non_rogue_wave_windows]) \
            and not df.iloc[potential_start:potential_end]['class'].eq(9).any():
            df.loc[potential_start:potential_end, 'class'] = 0
            non_rogue_wave_windows.append((potential_start, potential_end))
    
    return df


    # Apply the function
df_prepared_classified = prepare_and_classify_wave_data(df, sample_rate, waveHs_mapped)

# Saving the classified data
rogue_wave_data = df_prepared_classified[df_prepared_classified['class'] == 1]
non_rogue_wave_data = df_prepared_classified[df_prepared_classified['class'] == 0]

rogue_wave_file_path = 'rogue_wave_data.parquet'
non_rogue_wave_file_path = 'non_rogue_wave_data.parquet'
rogue_wave_data.to_parquet(rogue_wave_file_path)
non_rogue_wave_data.to_parquet(non_rogue_wave_file_path)

print(f"Rogue wave data saved to {rogue_wave_file_path}.")
print(f"Non-rogue wave data saved to {non_rogue_wave_file_path}.")
