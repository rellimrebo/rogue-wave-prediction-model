!pip install netCDF4 pandas
!pip install pyarrow
!pip install fastparquet
import netCDF4
import numpy as np
import datetime
import calendar
import pandas as pd
import math

# Configuration parameters
stn = '067'
deploy = '17'
start_date = '05/01/2008 00:00'  # MM/DD/YYYY HH:MM
duration = 30  # Duration in days

# Convert to unix timestamp
def get_unix_timestamp(human_time,dateFormat):
    unix_timestamp = int(calendar.timegm(datetime.datetime.strptime(human_time, dateFormat).timetuple()))
    return unix_timestamp

# Convert to human readable timestamp
def get_human_timestamp(unix_timestamp, dateFormat):
    human_timestamp = datetime.datetime.utcfromtimestamp(int(unix_timestamp)).strftime(dateFormat)
    return human_timestamp

# Load dataset
data_url = f'http://thredds.cdip.ucsd.edu/thredds/dodsC/cdip/archive/{stn}p1/{stn}p1_d{deploy}.nc'
nc = netCDF4.Dataset(data_url)
nc.set_auto_mask(False)

# Extracting variables
time_var = nc.variables['waveTime'][:]  # Assuming this is in UNIX timestamp format
waveHs = nc.variables['waveHs'][:]
start_time = nc.variables['xyzStartTime'][:].item()
sample_rate = nc.variables['xyzSampleRate'][:].item()
WaveHeight = nc.variables['xyzZDisplacement'][:]
filter_delay = nc.variables['xyzFilterDelay'][:]
end_time = start_time + (len(WaveHeight) / sample_rate)

# Calculate UNIX timestamps for analysis period
unix_start = get_unix_timestamp(start_date, "%m/%d/%Y %H:%M")
unix_end = unix_start + (duration * 24 * 60 * 60)  # Convert duration to seconds

# Calculate sub-second sample times
sample_time = np.arange(start_time, end_time, 1 / sample_rate)

# Finding indices for the analysis period
start_index = np.searchsorted(sample_time, unix_start)
end_index = np.searchsorted(sample_time, unix_end)

# Filter the data for the specified time range
filtered_wave_heights = WaveHeight[start_index:end_index]
filtered_timestamps = sample_time[start_index:end_index]

# Calculate the interval index for each filtered timestamp relative to waveHs
waveHs_interval_index = np.searchsorted(time_var, filtered_timestamps, side='right') - 1
waveHs_interval_index = np.clip(waveHs_interval_index, 0, len(waveHs) - 1)
waveHs_mapped = waveHs[waveHs_interval_index]

# Detecting rogue waves
is_rogue_wave = filtered_wave_heights > 2 * waveHs[waveHs_interval_index]
rogue_timestamps = filtered_timestamps[is_rogue_wave]
rogue_wave_heights = filtered_wave_heights[is_rogue_wave]
rogue_waveHs = waveHs[waveHs_interval_index][is_rogue_wave]

labels = np.zeros_like(filtered_timestamps, dtype=int)  # Initialize all as non-rogue waves


import os
import pandas as pd

def prepare_and_split_wave_data(filtered_timestamps, filtered_wave_heights, waveHs_mapped, sample_rate, observation_window=20*60, advance_warning_time=5*60):
    """
    Prepares rogue wave data with high precision, considering observation and advance warning times,
    and splits the dataset into rogue and non-rogue wave data.
    """
    df = pd.DataFrame({
        'timestamp': filtered_timestamps,
        'WaveHeight': filtered_wave_heights,
        'waveHs': waveHs_mapped,
        'is_rogue_wave': filtered_wave_heights > 2 * waveHs_mapped
    })

    # Initialize lists to hold rogue and non-rogue wave observation windows
    rogue_results = []
    non_rogue_results = []

    for idx, row in df.iterrows():
        if row['is_rogue_wave']:
            start_observation_time = row['timestamp'] - observation_window - advance_warning_time
            end_observation_time = row['timestamp']
            # Ensure the window is within the bounds of our dataset
            if start_observation_time >= df['timestamp'].min() and end_observation_time <= df['timestamp'].max():
                window_df = df[(df['timestamp'] >= start_observation_time) & (df['timestamp'] <= end_observation_time)]
                rogue_results.append(window_df)
        else:
            non_rogue_results.append(row)

    # Combine all rogue wave observation windows
    rogue_wave_data = pd.concat(rogue_results, ignore_index=True) if rogue_results else pd.DataFrame()
    non_rogue_wave_data = pd.DataFrame(non_rogue_results) if non_rogue_results else pd.DataFrame()

    return rogue_wave_data, non_rogue_wave_data

# Apply the function
rogue_wave_data, non_rogue_wave_data = prepare_and_split_wave_data(filtered_timestamps, filtered_wave_heights, waveHs_mapped, sample_rate)

# Define file paths
rogue_wave_file_path = 'rogue_wave_data.parquet'  # For saving rogue wave data
non_rogue_wave_file_path = 'non_rogue_wave_data.parquet'  # For saving non-rogue wave data

# Save the rogue wave data to a Parquet file
if not rogue_wave_data.empty:
    rogue_wave_data.to_parquet(rogue_wave_file_path)
    print(f"Rogue wave data has been saved to {os.path.abspath(rogue_wave_file_path)}.")

# Save the non-rogue wave data to a separate Parquet file
if not non_rogue_wave_data.empty:
    non_rogue_wave_data.to_parquet(non_rogue_wave_file_path)
    print(f"Non-rogue wave data has been saved to {os.path.abspath(non_rogue_wave_file_path)}.")



