import pyarrow.parquet as pq

def find_troughs_and_crests(displacement_data):
    crests, _ = find_peaks(displacement_data)
    troughs, _ = find_peaks(-displacement_data)
    return troughs, crests

def find_nearest_trough(troughs, crest):
    preceding_troughs = troughs[troughs < crest]
    return preceding_troughs.max() if preceding_troughs.size > 0 else None

def calculate_wave_heights(displacement_data, troughs, crests):
    wave_heights = []
    for crest in crests:
        nearest_trough = find_nearest_trough(troughs, crest)
        if nearest_trough is not None:
            height = displacement_data[crest] - displacement_data[nearest_trough]
            wave_heights.append((nearest_trough, crest, height))
    return wave_heights

def calculate_zero_upcrossings(displacement_data, sample_rate):
    zero_crossings = np.where(np.diff(np.sign(displacement_data)) > 0)[0]
    return len(zero_crossings), zero_crossings, np.mean(np.diff(zero_crossings)) / sample_rate if len(zero_crossings) > 1 else 0

def repetitive_value_checking(displacement_data):
    for i in range(len(displacement_data) - 10 + 1):
        if all(np.abs(displacement_data[i:i+10]) > 20.47):
            return True  # Indicates repetitive values exceeding threshold found
    return None
    
def excess_sensor_limit(displacement_data):
    if np.any(np.abs(displacement_data) > 20.47):
        return None

def should_discard_block(displacement_data, sample_rate, threshold):
    rate_of_change = np.abs(np.diff(displacement_data)) / (1/sample_rate)
    return np.any(rate_of_change > threshold)

def detect_non_rogue_wave(displacement_data, sample_rate, sigma):

    N_z, zero_upcrossing_indices, T_z = calculate_zero_upcrossings(displacement_data, sample_rate)
    if T_z == 0:  # Prevent division by zero
        return None
    S_y = (4 * sigma / T_z) * np.sqrt(2 * np.log(N_z))
    # Discard the block if the threshold is exceeded
    if should_discard_block(displacement_data, sample_rate, S_y) or repetitive_value_checking(displacement_data) or excess_sensor_limit(displacement_data):
        # print("Measurement discarded due to exceeding the rate of change threshold or repetitive)
        return None

    # Calculate the significant wave height (Hs)
    Hs = 4 * sigma

    # Calculate the troughs and crests in the window
    troughs, crests = find_troughs_and_crests(displacement_data)
    wave_heights_info = calculate_wave_heights(displacement_data, troughs, crests)

    # Verify all wave heights in the window are less than 2 * Hs
    for trough, crest, height in wave_heights_info:
        if height >= 2 * Hs:
            return False  # Window contains wave height >= 2 * Hs, not non-rogue

    # If we get here, all wave heights in the window are less than 2 * Hs
    return True



non_rogue_wave_data = pd.DataFrame(columns=['Station', 'Deployment', 'SamplingRate', 'Segment'])
last_station = None
start_time = time.time()

def process_station_deployment(station, deployment):
    data_url = f'http://thredds.cdip.ucsd.edu/thredds/dodsC/cdip/archive/{station}p1/{station}p1_d{deployment}.nc'
    nc = netCDF4.Dataset(data_url)
    nc.set_auto_mask(False)
    z_displacement = nc.variables['xyzZDisplacement'][:]
    sample_rate = nc.variables['xyzSampleRate'][:].item()
    sigma = np.std(z_displacement[np.abs(z_displacement) < 20.47])
    
    # Process and collect non-rogue blocks without enforcing a count limit here
    non_rogue_blocks = []
    for start_index in range(0, len(z_displacement) - int(30 * 60 * sample_rate), int(30 * 60 * sample_rate)):
        current_block = z_displacement[start_index:start_index + int(30 * 60 * sample_rate)]
        if detect_non_rogue_wave(current_block, sample_rate, sigma):
            non_rogue_blocks.append(current_block.tolist())

    return non_rogue_blocks, sample_rate


def main():
    start_time = time.time()
    non_rogue_wave_data_list = []
    total_target_blocks = 311

    stn = '132'
    deployment_list = [f"{i:02}" for i in range(1, 17)]
    
    for deployment in deployment_list:
        if len(non_rogue_wave_data_list) >= total_target_blocks:
            break  # Stop if the target number of blocks has been reached
        
        non_rogue_blocks, sample_rate = process_station_deployment(stn, deployment)
        for block in non_rogue_blocks:
            if len(non_rogue_wave_data_list) < total_target_blocks:
                non_rogue_wave_data_list.append({
                    'Station': stn,
                    'Deployment': deployment,
                    'SamplingRate': sample_rate,
                    'Segment': block
                })
            else:
                break  # Break the loop if we've collected enough blocks

    non_rogue_wave_data = pd.DataFrame(non_rogue_wave_data_list[:total_target_blocks])  # Ensure we only include up to the target number
    non_rogue_wave_data.to_parquet('non_rogue_wave_data.parquet')
    
    print('Processing completed.')
    print(f"--- {time.time() - start_time} seconds ---")

if __name__ == "__main__":
    main()
